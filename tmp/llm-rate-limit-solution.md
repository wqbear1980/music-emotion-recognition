# LLM 限流错误解决方案

## 错误信息
```
Error: 因触发限流调用内置集成失败，请稍后重试
Code: ErrTooManyRequests
Type: Forbidden
```

## 问题原因
系统当前使用**云端大语言模型**（豆包/Coze），但触发了API调用频率限制。

---

## 解决方案

### 🎯 方案1：切换到本地Ollama（强烈推荐）

#### 步骤1：在本地电脑配置Ollama

```bash
# 1. 启动Ollama服务（允许外部访问）
# macOS/Linux
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# Windows PowerShell
$env:OLLAMA_HOST="0.0.0.0:11434"; ollama serve

# 2. 拉取DeepSeek模型
ollama pull deepseek-r1:32b

# 3. 验证模型安装
ollama list

# 4. 测试模型运行
ollama run deepseek-r1:32b "你好"
```

#### 步骤2：获取你的电脑IP地址

```bash
# macOS/Linux
ifconfig | grep "inet " | grep -v 127.0.0.1

# Windows
ipconfig
```

记下你的IP地址，例如：`192.168.1.100`

#### 步骤3：在网页应用中配置

1. 打开「🤖 LLM 配置」面板
2. LLM模式：选择「本地（Local）」
3. 服务类型：选择「Ollama」
4. 基础URL：输入 `http://192.168.1.100:11434`（替换为你的IP）
5. 点击「检查连接」
6. 连接成功后，选择模型 `deepseek-r1:32b`
7. 点击「测试推理」
8. 点击「应用配置」

#### 步骤4：重新测试

配置完成后，重新上传音乐文件进行分析。

---

### ⏳ 方案2：等待限流解除（临时方案）

1. 等待5-10分钟
2. 减少请求频率
3. 稍后重试

---

## 为什么会出现限流？

云端大语言模型有调用频率限制：
- 短时间内多次调用会触发限流
- 限流通常在5-10分钟后自动解除
- 使用本地Ollama可以完全避免这个问题

---

## 优势对比

| 特性 | 云端LLM | 本地Ollama |
|------|---------|------------|
| 限流 | ❌ 有限流 | ✅ 无限流 |
| 成本 | ❌ 按次数收费 | ✅ 完全免费 |
| 隐私 | ⚠️ 数据上传云端 | ✅ 数据本地 |
| 速度 | ⚠️ 依赖网络 | ✅ 本地运行 |
| 配置难度 | ✅ 开箱即用 | ⚠️ 需要配置 |

---

## 常见问题

### Q: 找不到Ollama服务？
A: 确保本地电脑上Ollama正在运行，并使用 `OLLAMA_HOST=0.0.0.0:11434` 启动

### Q: 连接检查失败？
A: 检查防火墙设置，确保11434端口可访问

### Q: 模型推理速度慢？
A: 1) 使用更小的模型（如 deepseek-r1:8b）
      2) 确保电脑有足够内存
      3) 考虑使用GPU加速

### Q: 还想用云端怎么办？
A: 在「🤖 LLM 配置」中选择「云端（Cloud）」或「自动（Auto）」
